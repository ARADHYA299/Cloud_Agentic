# -*- coding: utf-8 -*-
"""Finops_Agentic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eu192-rpA0fEbao1FWD4J7mM6iVtoXTZ
"""

FINOPS_RULES = [
    {
        "rule_id" : "idle_compute",
        "condition" : "CPU utilization below 10% for more than 14 days",
        "action" : "Downsize or terminate the instance",
        "reason" : "Low utilization indicates overprovisioning",
        "risk" : "low"
    },

    {
        "rule_id" : "unattached_storage",
        "condition" : "Unattached storage volumes detected",
        "action" : "Delete or snapshot and archieve unused volumes",
        "reason" : "Unattached volumes incur cost without usage",
        "risk" : "low"
    },

    {
        "rule_id": "steady_workload",
        "condition": "Consistent usage for more than 30 days",
        "action": "Purchase reserved instances or savings plans",
        "reason": "Long-term usage benefits from discounted pricing",
        "risk": "medium"
    }
]

import random
import json

def generate_examples(rule , n = 20):
  examples = []
  for _ in range(n):
    cpu = random.randint(2,9)
    days = random.randint(14,60)
    cost = random.randint(500,6000)

    instruction = (
        f"Monthlt compute cost is ${cost}."
        f"Average CPU utilization is ${cpu}% for ${days} Days."
    )


    output = (
        f"{rule['action']}. "
        f"Reason : {rule['reason']}"
        f"Risk level : {rule['risk']}"
    )

    examples.append({
        "instruction" : instruction,
        "output" : output
    })
  return examples

dataset = []

for rule in FINOPS_RULES:
  dataset.extend(generate_examples(rule, n = 25))

with open("finops_train.jsonl" , "w") as f:
  for row in dataset:
    f.write(json.dumps(row) + "\n")

!pip install -q transformers peft accelerate bitsandbytes datasets

from huggingface_hub import login
login()

from transformers import AutoTokenizer, AutoModelForCausalLM


model_name = "microsoft/phi-3-mini-4k-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)


model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_4bit = True,
    device_map = "auto",
    trust_remote_code = True,
    use_auth_token = True
)

for name, module in model.named_modules():
    if "proj" in name or "attn" in name:
        print(name)

from peft import LoraConfig , get_peft_model


lora_config = LoraConfig(
    r = 8,
    lora_alpha = 16,
    target_modules = ["qkv_proj"],
    lora_dropout = 0.05,
    task_type = "CAUSAL_LM"
)

model = get_peft_model(model , lora_config)
model.print_trainable_parameters()

from datasets import load_dataset

dataset = load_dataset("json" , data_files = "finops_train.jsonl")["train"]

def tokenize(examples):
  prompt = f"Problem : {examples['instruction']}\nRecommendations:"
  return tokenizer(prompt, text_target = examples["output"], truncation = True , max_length = 512)

dataset = dataset.map(tokenize)

from transformers import TrainingArguments , Trainer

training_args = TrainingArguments(
    output_dir = "./finops-lora",
    per_device_train_batch_size = 2,
    gradient_accumulation_steps = 4,
    learning_rate = 2e-4,
    num_train_epochs = 2,
    fp16 = True,
    logging_steps = 10,
    save_strategy = "epoch",
    optim = "paged_adamw_32bit"
)

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = dataset
)


trainer.train()
